{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nickdg/anaconda3/envs/pres3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/nickdg/anaconda3/envs/pres3/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Each Data File into NumPy Arrays\n",
    "\n",
    "These files are found inside the \"Homework 3\" Directory, which can be downloaded from the github link.  The data was taken from a publicly-available dataset from Boston College.  In the study, students performed the Stroop Task (https://en.wikipedia.org/wiki/Stroop_effect), where they needed to say, as quickly as possible, the **color a word was printed in**, and ignore the **word itself**.  There are many subjects in this dataset, divided up into files that each contain a single array for a given experimental variable, with elements for every single trial the subjects performed:\n",
    "  - 'subject.txt': subject numbers\n",
    "  - 'trialnum.txt', trial number for a given session\n",
    "  - 'stimword.txt', what the stimulus word said (i.e. \"Y\" if the word was \"YELLOW\")\n",
    "  - 'stimcolor.txt': what color the stimulus was (this is what the subject was asked to report)\n",
    "  - 'respcolor.txt',: what color the subject reported.  \n",
    "  - 'reaction_time.txt': response time (msecs)\n",
    "  \n",
    "Please use the numpy.loadtxt() function to load these files.  The length of each of these arrays should be equal to each other.\n",
    "\n",
    "Note: for files containing strings, you'll need to set the dtype to **str** in the loadtxt function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homework Sample Data/Homework 3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = path.join('Homework Sample Data', 'Homework 3')\n",
    "dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Trials were there in total across all subjects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was the Maximum number of trials any subject did?  What was the Minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many subjects (i.e. Unique Subject IDs) were there in this study?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the mean response time, across all subjects and conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a histogram of the response times\n",
    "(Don't forget to make sure that **%matplotlib inline** has been run already in this notebook, or the plot may not show up!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform the Response Times\n",
    "\n",
    "You should see that the responses are not normally distributed (if you can't see this, try using a higher number of **bins** in your histogram function).  While this isn't really a problem, it's very convenient when you have normally-distributed data.  Let's try transforming it to get something a bit nicer for statistical analysis...\n",
    "\n",
    "  - Make a new response time array that is the **log** of the response times.  \n",
    "  - Plot a new histogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reject Bad Data\n",
    "\n",
    "Now, that's a little nicer!  But wait... there's are some trials that seem to be **outliers** (data points that someone decides aren't representative of the data set): those with **log response times less than 5.7**.  After much consideration and deliberation with my supervisor, I've decided to remove those trials from my whole analysis.\n",
    "\n",
    "Let's do this in a few steps:\n",
    "\n",
    "### Select the the trials that we want to **Keep** (Not the outliers)\n",
    "(Hint: You will be making a boolean array here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Logical Indexing to get the response times from those trials into a new array, effectively removing the bad response times.\n",
    "plot the histogram of the new log response time array to be sure those trials are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the bad trials from all of your other arrays, too.\n",
    "**Important:** Confirm that all of your arrays have the same length after filtering before moving on, so you can be sure that each index corresponds to the same trial!\n",
    "\n",
    "**Important:** It may be wise not to overwrite your old variable names.  While not a problem in principle, it can lead to issues when you try to re-run the same cell of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here on, use the Rejected data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall, what was the accuracy of the responses? \n",
    "  - How many trials in total were correct (stimcolor matched respcolor), \n",
    "  - how many were incorrect, and \n",
    "  - what percent accuracy was there, overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matched vs Unmatched Colors: \n",
    "\n",
    "  - What was the mean log response time for Congruent trials (the stimulus word and color matched each other)?  \n",
    "  - What was the mean log response time for Incongruent trials (when they did not match)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Bar Graph of Log Response Time\n",
    "The bar graph should show Mean Log Response Times for the Congruent and Incongruent conditions, with Standard-Deviation Error Bars.  Put text labels on the x access showing which bars go with which condition.\n",
    "\n",
    "For help and an example, take a look at http://matplotlib.org/examples/api/barchart_demo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Bar Graph of Response Time\n",
    "\n",
    "The logged data doesn't make a very good impression of just how big an effect this is, sadly.  Let's un-log it by taking the **exponent** (numpy.exp) of the logged response time data, and plot that bar graph to get the data in milliseconds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change in Performance over Time\n",
    "\n",
    "In experiments like this, we assume that each trial is independent of each other.  Often, this isn't the case, and not taking that into account can sometimes lead to some wrong conclusions.  Let's see what we can find...\n",
    "\n",
    "### How did reaction time change over the course of each subject's session? \n",
    "\n",
    "Were earlier trials faster or slower than later trials in a session, overall?  Make a scatter plot of the response times over the course of a session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about between each subject?  Do all subjects show this difference in response time between the Congruent and Incogruent conditions, or is this just coming from a few subjects?\n",
    "\n",
    "Make two arrays, one for each condition, which contains the mean log reaction times for each subject for that condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the difference in mean log reaction times between conditions for each subject!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think, should anyone be rejected?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participant Response Speed vs. Size of Stroop Effect\n",
    "\n",
    "Make a scatter plot showing the relationship between how quickly subjects responded for congruent and incongruent stimuli!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an interesting effect, don't you think? Anyway, I think that's enough analysis.  Let's publish that last figure and move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Figure to File\n",
    "\n",
    "Save the figure using **fig.savefig()** to your data directory as a png file!  Great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
